{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1Ld2+H3ZcnAlA202zuNuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muadmazahir/box-embedding-transformer/blob/main/Box_Embedding_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z1SDAs1p8FEk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Box Model and Utilities"
      ],
      "metadata": {
        "id": "fMjbUKV79PEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BOX UTILITIES AND WRAPPERS\n",
        "# =============================================================================\n",
        "\n",
        "euler_gamma = 0.57721566490153286060\n",
        "\n",
        "def _box_shape_ok(t: torch.Tensor, learnt_temp=False) -> bool:\n",
        "    if len(t.shape) < 2:\n",
        "        return False\n",
        "    if not learnt_temp:\n",
        "        if t.size(-2) != 2:\n",
        "            return False\n",
        "        return True\n",
        "    else:\n",
        "        if t.size(-2) != 4:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "def _shape_error_str(tensor_name, expected_shape, actual_shape):\n",
        "    return \"Shape of {} has to be {} but is {}\".format(\n",
        "        tensor_name, expected_shape, tuple(actual_shape)\n",
        "    )\n",
        "\n",
        "class BoxTensor(object):\n",
        "    \"\"\"A wrapper which contains single tensor which represents single or multiple boxes.\"\"\"\n",
        "\n",
        "    def __init__(self, data: torch.Tensor, learnt_temp: bool = False) -> None:\n",
        "        if _box_shape_ok(data, learnt_temp):\n",
        "            self.data = data\n",
        "        else:\n",
        "            raise ValueError(_shape_error_str(\"data\", \"(**,2,num_dims)\", data.shape))\n",
        "        super().__init__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"box_tensor_wrapper(\" + self.data.__repr__() + \")\"\n",
        "\n",
        "    @property\n",
        "    def z(self) -> torch.Tensor:\n",
        "        \"\"\"Lower left coordinate as Tensor\"\"\"\n",
        "        return self.data[..., 0, :]\n",
        "\n",
        "    @property\n",
        "    def Z(self) -> torch.Tensor:\n",
        "        \"\"\"Top right coordinate as Tensor\"\"\"\n",
        "        return self.data[..., 1, :]\n",
        "\n",
        "    @classmethod\n",
        "    def from_zZ(cls, z: torch.Tensor, Z: torch.Tensor):\n",
        "        \"\"\"Creates a box by stacking z and Z along -2 dim.\"\"\"\n",
        "        if z.shape != Z.shape:\n",
        "            raise ValueError(\n",
        "                \"Shape of z and Z should be same but is {} and {}\".format(\n",
        "                    z.shape, Z.shape\n",
        "                )\n",
        "            )\n",
        "        box_val: torch.Tensor = torch.stack((z, Z), -2)\n",
        "        return cls(box_val)\n",
        "\n",
        "    @classmethod\n",
        "    def from_split(cls, t: torch.Tensor, dim: int = -1):\n",
        "        \"\"\"Creates a BoxTensor by splitting on the dimension dim at midpoint\"\"\"\n",
        "        len_dim = t.size(dim)\n",
        "        if len_dim % 2 != 0:\n",
        "            raise ValueError(\n",
        "                \"dim has to be even to split on it but is {}\".format(t.size(dim))\n",
        "            )\n",
        "        split_point = int(len_dim / 2)\n",
        "        z = t.index_select(\n",
        "            dim,\n",
        "            torch.tensor(list(range(split_point)), dtype=torch.int64, device=t.device),\n",
        "        )\n",
        "        Z = t.index_select(\n",
        "            dim,\n",
        "            torch.tensor(\n",
        "                list(range(split_point, len_dim)), dtype=torch.int64, device=t.device\n",
        "            ),\n",
        "        )\n",
        "        return cls.from_zZ(z, Z)\n",
        "\n",
        "    def _intersection(self, other, gumbel_beta: float = 1.0, bayesian: bool = False):\n",
        "        t1 = self\n",
        "        t2 = other\n",
        "\n",
        "        if bayesian:\n",
        "            try:\n",
        "                z = gumbel_beta * torch.logaddexp(\n",
        "                    t1.z / gumbel_beta, t2.z / gumbel_beta\n",
        "                )\n",
        "                z = torch.max(z, torch.max(t1.z, t2.z))\n",
        "                Z = -gumbel_beta * torch.logaddexp(\n",
        "                    -t1.Z / gumbel_beta, -t2.Z / gumbel_beta\n",
        "                )\n",
        "                Z = torch.min(Z, torch.min(t1.Z, t2.Z))\n",
        "            except Exception as e:\n",
        "                print(\"Gumbel intersection is not possible\")\n",
        "                z = torch.max(t1.z, t2.z)\n",
        "                Z = torch.min(t1.Z, t2.Z)\n",
        "        else:\n",
        "            z = torch.max(t1.z, t2.z)\n",
        "            Z = torch.min(t1.Z, t2.Z)\n",
        "\n",
        "        return z, Z\n",
        "\n",
        "    def gumbel_intersection_log_volume(self, other, volume_temp=1.0, intersection_temp: float = 1.0, scale=1.0):\n",
        "        z, Z = self._intersection(other, gumbel_beta=intersection_temp, bayesian=True)\n",
        "        vol = self._log_soft_volume_adjusted(\n",
        "            z, Z, temp=volume_temp, gumbel_beta=intersection_temp, scale=scale\n",
        "        )\n",
        "        return vol\n",
        "\n",
        "    @classmethod\n",
        "    def _log_soft_volume(cls, z: torch.Tensor, Z: torch.Tensor, temp: float = 1.0, scale = 1.0) -> torch.Tensor:\n",
        "        eps = torch.finfo(z.dtype).tiny\n",
        "        if isinstance(scale, float):\n",
        "            s = torch.tensor(scale, dtype=z.dtype, device=z.device)\n",
        "        else:\n",
        "            s = scale\n",
        "        return torch.sum(\n",
        "            torch.log(F.softplus(Z - z, beta=temp) + 1e-23), dim=-1\n",
        "        ) + torch.log(torch.tensor(s, dtype=z.dtype, device=z.device))\n",
        "\n",
        "    def log_soft_volume(self, temp: float = 1.0, scale = 1.0) -> torch.Tensor:\n",
        "        res = self._log_soft_volume(self.z, self.Z, temp=temp, scale=scale)\n",
        "        return res\n",
        "\n",
        "    @classmethod\n",
        "    def _log_soft_volume_adjusted(cls, z: torch.Tensor, Z: torch.Tensor, temp: float = 1.0,\n",
        "                                gumbel_beta: float = 1.0, scale = 1.0) -> torch.Tensor:\n",
        "        eps = torch.finfo(z.dtype).tiny\n",
        "        if isinstance(scale, float):\n",
        "            s = torch.tensor(scale, dtype=z.dtype, device=z.device)\n",
        "        else:\n",
        "            s = scale\n",
        "        return (\n",
        "            torch.sum(\n",
        "                torch.log(\n",
        "                    F.softplus(Z - z - 2 * euler_gamma * gumbel_beta, beta=temp) + 1e-23\n",
        "                ),\n",
        "                dim=-1,\n",
        "            )\n",
        "            + torch.log(torch.tensor(s, dtype=z.dtype, device=z.device))\n",
        "        )\n",
        "\n",
        "    def intersection_log_soft_volume(self, other, temp: float = 1.0, gumbel_beta: float = 1.0,\n",
        "                                   bayesian: bool = False, scale = 1.0) -> torch.Tensor:\n",
        "        z, Z = self._intersection(other, gumbel_beta, bayesian)\n",
        "        vol = self._log_soft_volume(z, Z, temp=temp, scale=scale)\n",
        "        return vol\n",
        "\n",
        "    @classmethod\n",
        "    def get_wW(cls, z, Z):\n",
        "        return z, Z\n",
        "\n",
        "# =============================================================================\n",
        "# BOX EMBEDDING MODULE\n",
        "# =============================================================================\n",
        "\n",
        "def _uniform_small(weight, emb_dim, box_type):\n",
        "    \"\"\"\n",
        "    Creates a temporary tensor with uniform random values between 0.0 + 1e-7 and 0.9 - 1e-7\n",
        "    Sets the first half of the embedding (z) to these random values\n",
        "    Sets the second half (Z) to z + 0.1, creating small boxes with a fixed width of 0.1\n",
        "    Uses the box type's get_wW method to convert these (z, Z) coordinates into the appropriate weight representation\n",
        "    For BoxTensor: get_wW simply returns (z, Z) as-is\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        temp = torch.zeros_like(weight)\n",
        "        torch.nn.init.uniform_(temp, 0.0 + 1e-7, 1.0 - 0.1 - 1e-7)\n",
        "        z = temp[..., :emb_dim]\n",
        "        Z = z + 0.1\n",
        "        w, W = box_type.get_wW(z, Z)\n",
        "        weight[..., :emb_dim] = w\n",
        "        weight[..., emb_dim : emb_dim * 2] = W\n",
        "\n",
        "class BoxEmbedding(nn.Embedding):\n",
        "    \"\"\"BoxEmbedding is a wrapper around nn.Embedding.\n",
        "\n",
        "    It takes the provided embedding dimension and divides it by 2.\n",
        "    It initializes the weights using _uniform_small.\n",
        "    \"\"\"\n",
        "    box_types = {\"BoxTensor\": BoxTensor}\n",
        "\n",
        "    def init_weights(self):\n",
        "        _uniform_small(\n",
        "            self.weight,\n",
        "            self.box_embedding_dim,\n",
        "            self.box_types[self.box_type],\n",
        "        )\n",
        "\n",
        "    def __init__(self, num_embeddings: int, box_embedding_dim: int, box_type=\"BoxTensor\") -> None:\n",
        "        super().__init__(num_embeddings, box_embedding_dim)\n",
        "        self.box_type = box_type\n",
        "        self.box = self.box_types[box_type]\n",
        "        self.box_embedding_dim = box_embedding_dim // 2\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input: torch.Tensor):  # type: ignore\n",
        "        emb = super().forward(input) # (..., self.box_embedding_dim)\n",
        "        box_emb = self.box.from_split(emb) # (..., 2, self.box_embedding_dim // 2)\n",
        "        return box_emb\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# WORD2BOX MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class Word2Box(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=50, batch_size=10, n_gram=4,\n",
        "                 volume_temp=1.0, intersection_temp=1.0, box_type=\"BoxTensor\"):\n",
        "        super(Word2Box, self).__init__()\n",
        "\n",
        "        # Model parameters\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Box features\n",
        "        self.volume_temp = volume_temp\n",
        "        self.intersection_temp = intersection_temp\n",
        "        self.box_type = box_type\n",
        "\n",
        "        # Create embeddings\n",
        "        self.embeddings_word = BoxEmbedding(\n",
        "            self.vocab_size, self.embedding_dim, box_type=box_type\n",
        "        )\n",
        "        self.embeddings_context = BoxEmbedding(\n",
        "            self.vocab_size, self.embedding_dim, box_type=box_type\n",
        "        )\n",
        "\n",
        "    def forward(self, idx_word, idx_context):\n",
        "        # idx_word - (batch_size)\n",
        "        # idx_context - (batch_size, 1 + negative samples)\n",
        "        idx_word = idx_word.unsqueeze(1) # Broadcast the word vector to the the context + negative_samples. idx_word after unsqueezee - (batch_size, 1)\n",
        "        word_boxes = self.embeddings_word(idx_word) # (batch_size, 1, 2, embedding_dim//2)\n",
        "        context_boxes = self.embeddings_context(idx_context) # (batch_size, 1 + negative_samples, 2, embedding_dim//2)\n",
        "\n",
        "        if self.intersection_temp == 0.0:\n",
        "            score = word_boxes.intersection_log_soft_volume(\n",
        "                context_boxes, temp=self.volume_temp\n",
        "            )\n",
        "        else:\n",
        "            score = word_boxes.gumbel_intersection_log_volume(\n",
        "                context_boxes,\n",
        "                volume_temp=self.volume_temp,\n",
        "                intersection_temp=self.intersection_temp,\n",
        "            )\n",
        "        return score\n",
        "\n",
        "    @staticmethod\n",
        "    def max_margin_loss(pos, neg, margin=5.0):\n",
        "        \"\"\"Max margin loss for box embeddings\"\"\"\n",
        "        zero = torch.tensor(0.0, device=pos.device, dtype=pos.dtype)\n",
        "        return torch.sum(torch.max(zero, neg - pos + margin), dim=1)"
      ],
      "metadata": {
        "id": "89SGTyfk9S0R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfomer Building Bocks"
      ],
      "metadata": {
        "id": "_ChiqAJ3iqpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    vocab_size: int\n",
        "    n_layers: int = 4\n",
        "    n_heads: int = 4\n",
        "    d_model: int = 256\n",
        "    d_ff: int = 1024\n",
        "    max_seq_len: int = 64\n",
        "    attn_dropout: float = 0.1\n",
        "    resid_dropout: float = 0.1\n",
        "    emb_dropout: float = 0.1\n",
        "    device: str = \"cpu\"\n",
        "    num_negatives: int = 4\n",
        "    margin: int = 5.0\n",
        "    top_k_for_box: int = 32"
      ],
      "metadata": {
        "id": "0fJBubOMitsS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(d_model, eps=eps)\n",
        "    def forward(self, x):\n",
        "        return self.ln(x)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, attn_dropout: float, resid_dropout: float):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.n_heads = n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.proj = nn.Linear(d_model, d_model)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.resid_dropout = nn.Dropout(resid_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(C, dim=-1)\n",
        "        q = q.view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = k.view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = v.view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
        "        causal_mask = torch.tril(torch.ones(L, L, device=x.device)).view(1, 1, L, L)\n",
        "        attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "        y = attn_weights @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, L, C)\n",
        "        y = self.resid_dropout(self.proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(cfg.d_model)\n",
        "        self.attn = CausalSelfAttention(cfg.d_model, cfg.n_heads, cfg.attn_dropout, cfg.resid_dropout)\n",
        "        self.ln2 = LayerNorm(cfg.d_model)\n",
        "        self.mlp = MLP(cfg.d_model, cfg.d_ff, cfg.resid_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "YPhAT8cJixuF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "PHUvulb7i2kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BoxEmbeddingTransformer(nn.Module):\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.word2box = Word2Box(vocab_size=cfg.vocab_size, embedding_dim=cfg.d_model)\n",
        "        self.token_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
        "        self.token_emb.weight = self.word2box.embeddings_word.weight # tie token embs to word2box embs\n",
        "        self.pos_emb = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
        "        self.drop = nn.Dropout(cfg.emb_dropout)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "        self.ln_f = LayerNorm(cfg.d_model)\n",
        "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.word2box.embeddings_word.weight # tie lm_head weights to word2box embs\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, L = idx.shape\n",
        "        pos = torch.arange(0, L, device=idx.device).unsqueeze(0)\n",
        "        x = self.token_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # logits - (batch_size, block_size, vocab_size)\n",
        "\n",
        "        total_loss = None\n",
        "        if targets is not None:\n",
        "          # probs over vocab\n",
        "          probs = torch.softmax(logits, dim=-1)  # (B, L, V)\n",
        "\n",
        "          # top-K per position\n",
        "          topk_probs, topk_idx = torch.topk(probs, self.cfg.top_k_for_box, dim=-1) # (B, L, K)\n",
        "\n",
        "          # renormalize over K (safer with clamp)\n",
        "          denom = topk_probs.sum(dim=-1, keepdim=True).clamp_min(1e-12)\n",
        "          topk_probs = topk_probs / denom   # (B, L, K)\n",
        "\n",
        "          # flatten candidate ids\n",
        "          BL = B * L\n",
        "          cand_idx = topk_idx.reshape(-1)  # (BL*K,)\n",
        "\n",
        "          # build contexts aligned to (BL, 1+N) then repeat K times\n",
        "          ctx = targets.view(BL, targets.size(-1))  # (BL, 1+N)\n",
        "          ctx_rep = ctx.unsqueeze(1).expand(BL, self.cfg.top_k_for_box, ctx.size(-1)).reshape(BL * self.cfg.top_k_for_box, ctx.size(-1)) # (BL*K, 1+N)\n",
        "\n",
        "          # score each candidate token against its context\n",
        "          scores = self.word2box(idx_word=cand_idx, idx_context=ctx_rep)  # (BL*K, 1+N)\n",
        "\n",
        "          # split to pos/neg\n",
        "          N = self.cfg.num_negatives\n",
        "          assert scores.size(-1) == 1 + N, f\"scores last dim {scores.size(-1)} != 1+N\"\n",
        "          pos = scores[..., 0].view(BL * self.cfg.top_k_for_box, 1) # (BLK, 1)\n",
        "          neg = scores[..., 1:].view(BL * self.cfg.top_k_for_box, N) # (BLK, N)\n",
        "\n",
        "          # per-candidate margin loss -> (BLK,)\n",
        "          loss_cand = self.word2box.max_margin_loss(pos, neg, margin=self.cfg.margin)\n",
        "\n",
        "          # expectation over top-K\n",
        "          loss_cand = loss_cand.view(BL, self.cfg.top_k_for_box)  # (BL, K)\n",
        "          weights   = topk_probs.view(BL, self.cfg.top_k_for_box) # (BL, K)\n",
        "          loss_bl   = (weights * loss_cand).sum(dim=-1)  # (BL,)\n",
        "\n",
        "          # mean\n",
        "          total_loss = loss_bl.mean()\n",
        "\n",
        "        return logits, total_loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50, temperature=0.8):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.cfg.max_seq_len:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "LcvqLThdi3wm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation + Model Setup"
      ],
      "metadata": {
        "id": "hJTQPtMaNc6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "_8hFsX_y1pSF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 1) Load a small external dataset (WikiText-2 raw)\n",
        "# -------------------------\n",
        "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "train_texts = ds[\"train\"][\"text\"]\n",
        "val_texts = ds[\"validation\"][\"text\"]\n",
        "\n",
        "# Filter out empty lines to avoid tons of EOS tokens\n",
        "train_texts = [t for t in train_texts if t and not t.isspace()]\n",
        "val_texts = [t for t in val_texts if t and not t.isspace()]\n",
        "\n",
        "BASE_MODEL_NAME = \"google/gemma-2-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Encode dataset and Calculate distribution\n",
        "# -------------------------\n",
        "\n",
        "def encode_and_build_dist(texts, power=0.75):\n",
        "    from collections import Counter\n",
        "    ids = []\n",
        "    counter = Counter()\n",
        "\n",
        "    for t in texts:\n",
        "        # encode each text (without special tokens, unless you want them counted)\n",
        "        token_ids = tokenizer.encode(t, add_special_tokens=False)\n",
        "\n",
        "        # extend the global list of IDs\n",
        "        ids.extend(token_ids)\n",
        "\n",
        "        # update counts at the same time\n",
        "        counter.update(token_ids)\n",
        "\n",
        "    # Convert IDs into a tensor\n",
        "    ids = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "    # Build frequency tensor\n",
        "    vocab_size = len(tokenizer)\n",
        "    freqs = torch.zeros(vocab_size, dtype=torch.float)\n",
        "    for idx, count in counter.items():\n",
        "        freqs[idx] = count\n",
        "\n",
        "    # Apply 3/4 smoothing (word2vec trick)\n",
        "    freqs = freqs.pow(power)\n",
        "\n",
        "    # Normalize to get a probability distribution\n",
        "    dist = freqs / freqs.sum()\n",
        "\n",
        "    return ids, dist\n",
        "\n",
        "train_ids, train_dist = encode_and_build_dist(train_texts)\n",
        "val_ids, val_dist     = encode_and_build_dist(val_texts)\n"
      ],
      "metadata": {
        "id": "59fe-ag0NZDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 4) Batching utility for contiguous language modeling\n",
        "# -------------------------\n",
        "block_size = 128  # context window\n",
        "batch_size = 12\n",
        "negative_samples = 3\n",
        "\n",
        "def make_batch(ids: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    assert ids.numel() > block_size + 1\n",
        "    ix = torch.randint(0, ids.numel() - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([ids[i:i + block_size] for i in ix])\n",
        "    y = torch.stack([ids[i + 1:i + block_size + 1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "def add_negatives(\n",
        "    y_true: torch.Tensor,           # shape - (batch_size, block_size)\n",
        "    sampling_distn: torch.Tensor,   # shape - (vocab_size,) non-negative weights\n",
        "    num_negatives: int\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns (batch_size, block_size, 1+num_negatives) tensor:\n",
        "      - [:, :, 0] is the true target id\n",
        "      - [:, :, 1:] are sampled negatives\n",
        "    \"\"\"\n",
        "    batch_size, block_size = y_true.shape\n",
        "    vocab_size = sampling_distn.numel()\n",
        "\n",
        "    # Expand distribution per position and zero-out the true id\n",
        "    p = sampling_distn.unsqueeze(0).expand(batch_size * block_size, vocab_size).clone()\n",
        "    true_flat = y_true.reshape(-1, 1).long()\n",
        "    p.scatter_(1, true_flat, 0)\n",
        "\n",
        "    # Normalize so each row sums to 1\n",
        "    p /= p.sum(dim=1, keepdim=True)\n",
        "\n",
        "    # Sample negatives\n",
        "    negs = torch.multinomial(p, num_samples=num_negatives, replacement=True)  # (batch_size * block_size, num_negatives)\n",
        "    negs = negs.view(batch_size, block_size, num_negatives)\n",
        "\n",
        "    # Concatenate true ids\n",
        "    y_all = torch.cat([y_true.unsqueeze(-1), negs], dim=-1)  # (batch_size * block_size, 1+num_negatives)\n",
        "    return y_all"
      ],
      "metadata": {
        "id": "oaT-7WnyOSLs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 5) Configure and build model\n",
        "# -------------------------\n",
        "cfg = GPTConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    n_layers=4,\n",
        "    n_heads=4,\n",
        "    d_model=256,\n",
        "    d_ff=1024,\n",
        "    max_seq_len=block_size,\n",
        "    attn_dropout=0.1,\n",
        "    resid_dropout=0.1,\n",
        "    emb_dropout=0.1,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    num_negatives=4,\n",
        "    margin=5.0,\n",
        "    top_k_for_box = 32\n",
        ")\n",
        "\n",
        "model = BoxEmbeddingTransformer(cfg)\n",
        "model.to(cfg.device)\n",
        "\n",
        "# -------------------------\n",
        "# 6) Optimizer\n",
        "# -------------------------\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)"
      ],
      "metadata": {
        "id": "RQiijgxiOVtp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "2-3SZ7zGOCGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 7) Training loop\n",
        "# -------------------------\n",
        "steps = 1400  # modest run on a small dataset\n",
        "eval_every = 200\n",
        "grad_clip = 1.0\n",
        "\n",
        "model.train()\n",
        "for step in range(1, steps + 1):\n",
        "    x, y = make_batch(train_ids) # x and y - (batch_size, block_size)\n",
        "    y_new = add_negatives(y, train_dist, cfg.num_negatives)  # y_new - (batch_size, block_size, 1+cfg.num_negatives)\n",
        "    x = x.to(model.cfg.device)\n",
        "    y_new = y_new.to(model.cfg.device)\n",
        "\n",
        "    _, loss = model(x, targets=y_new)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    if grad_clip is not None:\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % eval_every == 0 or step == 1:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            vx, vy = make_batch(val_ids)\n",
        "            vy_new = add_negatives(vy, val_dist, cfg.num_negatives)\n",
        "            vx = vx.to(model.cfg.device)\n",
        "            vy_new = vy_new.to(model.cfg.device)\n",
        "            _, vloss = model(vx, targets=vy_new)\n",
        "        print(f\"step {step:4d} | train loss {loss} | val loss {vloss}\")\n",
        "        model.train()"
      ],
      "metadata": {
        "id": "F7Yrj45cOBzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "I5V97mtg4DZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e540a3-16c8-4546-bebd-4d86f08d23ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"/content/drive/MyDrive/box-embedding-transformer-model.pth\")"
      ],
      "metadata": {
        "id": "9zmnfhYL37Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model\n",
        "model = torch.load(\"/content/drive/MyDrive/box-embedding-transformer-model.pth\", map_location=cfg.device, weights_only=False,)"
      ],
      "metadata": {
        "id": "jPcP-hLw3-hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# 8) Sample a short completion\n",
        "# -------------------------\n",
        "model.eval()\n",
        "prompt_text = \"Wikipedia is a free online\"\n",
        "prompt_ids = tokenizer.encode(prompt_text)\n",
        "prompt = torch.tensor(prompt_ids, dtype=torch.long, device=cfg.device).unsqueeze(0)\n",
        "out_ids = model.generate(prompt, max_new_tokens=10)\n",
        "\n",
        "# Decode (strip any double BOS/EOS artifacts from processor)\n",
        "decoded = tokenizer.decode(out_ids[0].tolist(), skip_special_tokens=True)\n",
        "print(out_ids[0][36])"
      ],
      "metadata": {
        "id": "f6nbv12722zY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}